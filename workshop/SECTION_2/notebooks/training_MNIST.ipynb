{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;, font-style: strong;\">Partie 2 : MNIST with Multiple Layer Perceptron (MLP)</p>\n",
    "\n",
    "### <p style=\"text-align: center;\">(Almond 0.9.1, Scala 2.12.10)</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will work on a dataset popularly known as MNIST. It is a dataset consisting of handwritten digits. More details can be found here: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "We will train a model to recognize the handwritten digits into correct digit:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../resources/mnist_mlp.png\" alt=\"MNIST Dataset and Number Classification\" style=\"width: 800px;\"/>  \n",
    "\n",
    "Ref: https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Usual suspects in Scala TF setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interp.load.ivy(coursierapi.Dependency.of(\"org.platanios\", \"tensorflow_2.12\", \"0.4.1\").withClassifier(\"linux-cpu-x86_64\"))\n",
    "interp.load.ivy(\"org.platanios\" %% \"tensorflow-data\" % \"0.4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import java.nio.file.Paths\n",
    "\n",
    "import org.platanios.tensorflow.api._\n",
    "\n",
    "import org.platanios.tensorflow.api.tf\n",
    "import org.platanios.tensorflow.api.tensors.Tensor\n",
    "import org.platanios.tensorflow.api.core.Shape\n",
    "import org.platanios.tensorflow.api.core.Indexer._\n",
    "import org.platanios.tensorflow.api.core.client.Session\n",
    "import org.platanios.tensorflow.data.image.MNISTLoader\n",
    "\n",
    "import org.platanios.tensorflow.api.learn.layers.{ Flatten, Input, Linear, ReLU, SparseSoftmaxCrossEntropy, Mean }\n",
    "import org.platanios.tensorflow.api.learn.{ Model, StopCriteria }\n",
    "import org.platanios.tensorflow.api.learn.estimators.InMemoryEstimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display MNIST Dataset\n",
    "\n",
    "Define and execute a function to display the first 20 images found in the written digits images database.  \n",
    "MNIST dataset is already downloaded in resource directory from http://yann.lecun.com/exdb/mnist/  \n",
    "We will use `org.platanios.tensorflow.data.image.MNISTLoader` class to load the dataset.  \n",
    "\n",
    "Original image is of `28 x 28` dimension with `grayscale` channel. And use `tf.image.encodePng` to transform it into PNG image.  \n",
    "Displayed image is resized into size of `100 x 100`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "{{\n",
    "def displayNumberMNIST(nb: Int) {\n",
    "    val dataset = MNISTLoader.load(Paths.get(\"../resources/dataset\"))\n",
    "    val images = dataset.trainImages\n",
    "    val imagesToDisplay = images.slice(0 :: nb, ::, ::)\n",
    "    for (index <- 0 until nb) {\n",
    "        val png = Session().run(fetches = tf.decodeRaw[Byte](tf.image.encodePng(imagesToDisplay(index).reshape(Shape(28, 28, 1)))))\n",
    "        Image(png.entriesIterator.toArray).withFormat(Image.PNG).withWidth(100).withHeight(100).display \n",
    "    }\n",
    "}\n",
    "displayNumberMNIST(20)\n",
    "}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset details\n",
    "1. __train-images-idx3-ubyte.gz__: 60k images of dim 28x28 for training the network\n",
    "2. __train-labels-idx1-ubyte.gz__: 60k labels (digit between 0 and 9: output of network) for above training images\n",
    "3. __t10k-images-idx3-ubyte.gz__: 10k images of dim. 28x28 for testing purpose\n",
    "4. __t10k-labels-idx1-ubyte.gz__: 10k labels for test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val dataset = MNISTLoader.load(Paths.get(\"../resources/dataset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for training\n",
    "\n",
    "We need to combine training image and its label to create a  proper training dataset for the network.  \n",
    "We do some preprocessing e.g. shuffling, iteraor looping, batching for  training.  \n",
    "We create batches of 256 images at a time, so that each training iteration will use 256 images.  \n",
    "We are using tensorflow `tf.data.Dataset` class API to do the above data processing:  \n",
    "\n",
    "- `zip`: creates a new dataset by zipping training image and its label  \n",
    "- `repeat`: repeast the dataset whenever the network ask new data for training and the network has already seen all the dataset  \n",
    "- `shuffle`: it will shuffle the dataset  \n",
    "- `batch`: combines consecutive elements of the dataset into one element, so that in each iteration when the network ask for data, it will get # of data defined in a batch  \n",
    "- `prefetch`: prefetches elements from the dataset  \n",
    "\n",
    "\n",
    "For more details follow below references:  \n",
    "  - https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "  - https://www.tensorflow.org/guide/data\n",
    "  - https://adventuresinmachinelearning.com/tensorflow-dataset-tutorial/\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val dataset = MNISTLoader.load(Paths.get(\"../resources/dataset\"))\n",
    "val trainImages = tf.data.datasetFromTensorSlices(dataset.trainImages.toFloat)\n",
    "\n",
    "val trainLabels = tf.data.datasetFromTensorSlices(dataset.trainLabels.toLong)\n",
    "val trainData =\n",
    "  trainImages.zip(trainLabels)\n",
    "      .repeat()\n",
    "      .shuffle(60000)\n",
    "      .batch(256)\n",
    "      .prefetch(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "We define here the shape for input data and the Neural Network topology.\n",
    "\n",
    "`Flatten[Float](\"Input/Flatten\")` layer: We start by reshaping the `28x28` matrix as a flat vector of size 784.\n",
    "\n",
    "`Linear[Float](\"Layer_0\", 256)` layer: Then we connect these cells to a single 256 nodes layer, fully connected.\n",
    "\n",
    "`Linear[Float](\"OutputLayer\", 10)` layer: Then again connect these to a 10-cells output (because we have 10 classes = 10 digits)\n",
    "\n",
    "As shown in the diagram below:\n",
    "\n",
    "<img src=\"../resources/mnist_2layers_new.png\" alt=\"MNIST Dataset and Number Classification\" style=\"width: 600px;\"/>  \n",
    "\n",
    "Ref: https://ml4a.github.io/ml4a/looking_inside_neural_nets/\n",
    "\n",
    "\n",
    "\n",
    "### To Do: Excercise\n",
    "\n",
    "- Add a second fully connected Layer\n",
    "- test the use of a Rectifying Linear Unit at each layer output like `ReLU[Float](\"Layer_0/Activation\")`\n",
    "- test more steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minput\u001b[39m: \u001b[32mInput\u001b[39m[\u001b[32mOutput\u001b[39m[\u001b[32mFloat\u001b[39m]] = org.platanios.tensorflow.api.learn.layers.Input@71396a61\n",
       "\u001b[36mtrainInput\u001b[39m: \u001b[32mInput\u001b[39m[\u001b[32mOutput\u001b[39m[\u001b[32mLong\u001b[39m]] = org.platanios.tensorflow.api.learn.layers.Input@6b1c2178\n",
       "\u001b[36mlayer\u001b[39m: \u001b[32mlearn\u001b[39m.\u001b[32mlayers\u001b[39m.\u001b[32mCompose\u001b[39m[\u001b[32mOutput\u001b[39m[\u001b[32mFloat\u001b[39m], \u001b[32mOutput\u001b[39m[\u001b[32mFloat\u001b[39m], \u001b[32mOutput\u001b[39m[\u001b[32mFloat\u001b[39m]] = \u001b[33mCompose\u001b[39m(\n",
       "  \u001b[32m\"Input/Flatten\"\u001b[39m,\n",
       "  \u001b[33mCompose\u001b[39m(\n",
       "    \u001b[32m\"Input/Flatten\"\u001b[39m,\n",
       "    \u001b[33mFlatten\u001b[39m(\u001b[32m\"Input/Flatten\"\u001b[39m),\n",
       "    \u001b[33mLinear\u001b[39m(\n",
       "      \u001b[32m\"Layer_0\"\u001b[39m,\n",
       "      \u001b[32m256\u001b[39m,\n",
       "      true,\n",
       "      \u001b[33mRandomNormalInitializer\u001b[39m(Tensor[Float, []], Tensor[Float, []], \u001b[32mNone\u001b[39m),\n",
       "      \u001b[33mRandomNormalInitializer\u001b[39m(Tensor[Float, []], Tensor[Float, []], \u001b[32mNone\u001b[39m)\n",
       "    )\n",
       "  ),\n",
       "  \u001b[33mLinear\u001b[39m(\n",
       "    \u001b[32m\"OutputLayer\"\u001b[39m,\n",
       "    \u001b[32m10\u001b[39m,\n",
       "    true,\n",
       "    \u001b[33mRandomNormalInitializer\u001b[39m(Tensor[Float, []], Tensor[Float, []], \u001b[32mNone\u001b[39m),\n",
       "    \u001b[33mRandomNormalInitializer\u001b[39m(Tensor[Float, []], Tensor[Float, []], \u001b[32mNone\u001b[39m)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create the MLP model.\n",
    "val input = Input(FLOAT32, Shape(-1, 28, 28))\n",
    "val trainInput = Input(INT64, Shape(-1))\n",
    "val layer = Flatten[Float](\"Input/Flatten\") >> \n",
    "    Linear[Float](\"Layer_0\", 256)  >>\n",
    "    Linear[Float](\"OutputLayer\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, optimization and wrapping in an Estimator\n",
    "\n",
    "- __Loss__: For loss calculation, we are using softmax with cross entropy combined layer together with a mean layer.  \n",
    "Measures the probability error in discrete classification tasks in which the classes are mutually exclusive.  \n",
    "Details: https://www.tensorflow.org/api_docs/python/tf/compat/v1/losses/sparse_softmax_cross_entropy\n",
    "\n",
    "\n",
    "- __Optimizer__: Optimizer that implements the Adam algorithm.\n",
    "Details: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam\n",
    "\n",
    "- __Model__: Define a supervised model with its required parameters\n",
    "\n",
    "- __InMemoryEstimator__: In-memory estimator which is used to train, use, and evaluate TensorFlow models, and uses an underlying TensorFlow session that it keeps alive throughout its lifetime.  \n",
    "Details: [InMemoryEstimator.scala](https://github.com/eaplatanios/tensorflow_scala/blob/master/modules/api/src/main/scala/org/platanios/tensorflow/api/learn/estimators/InMemoryEstimator.scala)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val loss = SparseSoftmaxCrossEntropy[Float, Long, Float](\"Loss\") >> Mean(\"Loss/Mean\")\n",
    "val optimizer = tf.train.Adam()\n",
    "val model = Model.simpleSupervised(input, trainInput, layer, loss, optimizer)\n",
    "\n",
    "// Create an estimator and train the model.\n",
    "val estimator = InMemoryEstimator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training!\n",
    "Train the model and stop it after some no. of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(() => trainData, StopCriteria(maxSteps = Some(2500)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for model quality: accuracy\n",
    "Evaluate the model's accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(images: Tensor[UByte], labels: Tensor[UByte]): Float = {\n",
    "    val predictions = estimator.infer(() => images.toFloat)\n",
    "    predictions\n",
    "      .argmax(1).toUByte\n",
    "      .equal(labels).toFloat\n",
    "      .mean().scalar\n",
    "}\n",
    "\n",
    "println(s\"Train accuracy = ${accuracy(dataset.trainImages, dataset.trainLabels)}\")\n",
    "println(s\"Test accuracy = ${accuracy(dataset.testImages, dataset.testLabels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results\n",
    "Check the performance of trained model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val images = dataset.testImages\n",
    "\n",
    "def inferOnSelectedImage(indexes: Seq[Int], images: Tensor[UByte]) {\n",
    "    indexes.foreach { index => \n",
    "        val imageToInfer = images.slice(index, ::, ::).reshape(Shape(1, 28, 28))\n",
    "        val predictions = estimator.infer(() => imageToInfer.toFloat)\n",
    "        println(s\"Label infered: ${predictions.argmax(1).scalar}\")\n",
    "        val png = Session().run(fetches = tf.decodeRaw[Byte](tf.image.encodePng(imageToInfer.reshape(Shape(28, 28, 1)))))\n",
    "        Image(png.entriesIterator.toArray).withFormat(Image.PNG).withWidth(100).withHeight(100).display \n",
    "    }\n",
    "}\n",
    "\n",
    "inferOnSelectedImage((30 to 40), images)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
